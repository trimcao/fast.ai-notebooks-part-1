{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX TITAN X (0000:04:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "%matplotlib inline\n",
    "from importlib import reload  # Python 3\n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/dogscats/\"\n",
    "# path = \"data/dogscats/sample/\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)\n",
    "\n",
    "batch_size=128\n",
    "# batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're going to create an ensemble of models and use their average as our predictions. For each ensemble, we're going to follow our usual fine-tuning steps:\n",
    "\n",
    "1) Create a model that retrains just the last layer\n",
    "2) Add this to a model containing all VGG layers except the last layer\n",
    "3) Fine-tune just the dense layers of this model (pre-computing the convolutional layers)\n",
    "4) Add data augmentation, fine-tuning the dense layers without pre-computation.\n",
    "\n",
    "So first, we need to create our VGG model and pre-compute the output of the conv layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Vgg16().model\n",
    "conv_layers,fc_layers = split_at(model, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_features = conv_model.predict_generator(val_batches, int(np.ceil(val_batches.samples/batch_size)))\n",
    "trn_features = conv_model.predict_generator(batches, int(np.ceil(batches.samples/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path + 'train_convlayer_features.bc', trn_features)\n",
    "save_array(model_path + 'valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can just load these precomputed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(model_path+'train_convlayer_features.bc')\n",
    "val_features = load_array(model_path+'valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save some time by pre-computing the training and validation arrays with the image decoding and resizing already done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path+'train')\n",
    "val = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_data.bc', trn)\n",
    "save_array(model_path+'valid_data.bc', val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can just load these resized images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = load_array(model_path+'train_data.bc')\n",
    "val = load_array(model_path+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can precompute the output of all but the last dropout and dense layers, for creating the first stage of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ll_val_feat = model.predict_generator(val_batches, int(np.ceil(val_batches.samples/batch_size)))\n",
    "ll_feat = model.predict_generator(batches, int(np.ceil(batches.samples/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path + 'train_ll_feat.bc', ll_feat)\n",
    "save_array(model_path + 'valid_ll_feat.bc', ll_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ll_feat = load_array(model_path+ 'train_ll_feat.bc')\n",
    "ll_val_feat = load_array(model_path + 'valid_ll_feat.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and let's also grab the test data, for when we need to submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test = get_data(path+'test')\n",
    "save_array(model_path+'test_data.bc', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = load_array(model_path+'test_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions automate creating a model that trains the last layer from scratch, and then adds those new layers on to the main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ll_layers():\n",
    "    return [ \n",
    "        BatchNormalization(input_shape=(4096,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax') \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_last_layer(i):\n",
    "    ll_layers = get_ll_layers()\n",
    "    ll_model = Sequential(ll_layers)\n",
    "    ll_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    ll_model.optimizer.lr=1e-5\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), epochs=12)\n",
    "    ll_model.optimizer.lr=1e-7\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), epochs=1)\n",
    "    ll_model.save_weights(model_path+'ll_bn' + i + '.h5')\n",
    "\n",
    "    vgg = Vgg16BN()\n",
    "    model = vgg.model\n",
    "    model.pop(); model.pop(); model.pop()\n",
    "    for layer in model.layers: layer.trainable=False\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    ll_layers = get_ll_layers()\n",
    "    for layer in ll_layers: model.add(layer)\n",
    "    for l1,l2 in zip(ll_model.layers, model.layers[-3:]):\n",
    "        l2.set_weights(l1.get_weights())\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save_weights(model_path+'bn' + i + '.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_model(model):\n",
    "    layers = model.layers\n",
    "    last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                         if type(layer) is Convolution2D][-1]\n",
    "\n",
    "    conv_layers = layers[:last_conv_idx+1]\n",
    "    conv_model = Sequential(conv_layers)\n",
    "    fc_layers = layers[last_conv_idx+1:]\n",
    "    return conv_model, fc_layers, last_conv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_layers(p, in_shape):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=in_shape),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(2, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dense_layers(i, model):\n",
    "    conv_model, fc_layers, last_conv_idx = get_conv_model(model)\n",
    "    conv_shape = conv_model.output_shape[1:]\n",
    "    fc_model = Sequential(get_fc_layers(0.5, conv_shape))\n",
    "    for l1,l2 in zip(fc_model.layers, fc_layers): \n",
    "        weights = l2.get_weights()\n",
    "        l1.set_weights(weights)\n",
    "    fc_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "    fc_model.fit(trn_features, trn_labels, epochs=2, \n",
    "         batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "\n",
    "    # width_zoom_range removed from the following because not available in Keras2\n",
    "    gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, zoom_range=0.05,\n",
    "       channel_shift_range=10, height_shift_range=0.05, shear_range=0.05, horizontal_flip=True)\n",
    "    batches = gen.flow(trn, trn_labels, batch_size=batch_size)\n",
    "    val_batches = image.ImageDataGenerator().flow(val, val_labels, \n",
    "                      shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    for layer in conv_model.layers: layer.trainable = False\n",
    "    for layer in get_fc_layers(0.5, conv_shape): conv_model.add(layer)\n",
    "    for l1,l2 in zip(conv_model.layers[last_conv_idx+1:], fc_model.layers): \n",
    "        l1.set_weights(l2.get_weights())\n",
    "\n",
    "    steps_per_epoch = int(np.ceil(batches.n/batch_size))\n",
    "    validation_steps = int(np.ceil(val_batches.n/batch_size))\n",
    "\n",
    "    conv_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                       metrics=['accuracy'])\n",
    "    conv_model.save_weights(model_path+'no_dropout_bn' + i + '.h5')\n",
    "    conv_model.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=1, \n",
    "                            validation_data=val_batches, validation_steps=validation_steps)\n",
    "    for layer in conv_model.layers[16:]: layer.trainable = True\n",
    "    conv_model.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=8, \n",
    "                            validation_data=val_batches, validation_steps=validation_steps)\n",
    "\n",
    "    conv_model.optimizer.lr = 1e-7\n",
    "    conv_model.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=10, \n",
    "                            validation_data=val_batches, validation_steps=validation_steps)\n",
    "    conv_model.save_weights(model_path + 'aug' + i + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.8578 - acc: 0.6804 - val_loss: 2.2534 - val_acc: 0.3120\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6839 - acc: 0.7794 - val_loss: 2.4996 - val_acc: 0.2990\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6496 - acc: 0.7960 - val_loss: 2.4651 - val_acc: 0.2970\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6391 - acc: 0.7963 - val_loss: 2.4541 - val_acc: 0.2965\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6348 - acc: 0.8000 - val_loss: 2.4300 - val_acc: 0.2945\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6146 - acc: 0.7994 - val_loss: 2.3874 - val_acc: 0.2950\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6045 - acc: 0.8013 - val_loss: 2.3943 - val_acc: 0.2920\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5854 - acc: 0.8081 - val_loss: 2.3667 - val_acc: 0.2930\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5955 - acc: 0.8052 - val_loss: 2.3389 - val_acc: 0.2930\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5776 - acc: 0.8110 - val_loss: 2.2973 - val_acc: 0.2920\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5655 - acc: 0.8106 - val_loss: 2.2377 - val_acc: 0.2935\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5638 - acc: 0.8119 - val_loss: 2.2357 - val_acc: 0.2895\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 1s - loss: 0.5568 - acc: 0.8107 - val_loss: 2.2523 - val_acc: 0.2910\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0881 - acc: 0.9671 - val_loss: 0.0441 - val_acc: 0.9855\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0414 - acc: 0.9862 - val_loss: 0.0419 - val_acc: 0.9850\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 194s - loss: 0.0536 - acc: 0.9815 - val_loss: 0.0425 - val_acc: 0.9850\n",
      "Epoch 1/8\n",
      "180/180 [==============================] - 191s - loss: 0.0429 - acc: 0.9835 - val_loss: 0.0487 - val_acc: 0.9830\n",
      "Epoch 2/8\n",
      "180/180 [==============================] - 189s - loss: 0.0344 - acc: 0.9876 - val_loss: 0.0471 - val_acc: 0.9860\n",
      "Epoch 3/8\n",
      "180/180 [==============================] - 188s - loss: 0.0242 - acc: 0.9913 - val_loss: 0.0460 - val_acc: 0.9855\n",
      "Epoch 4/8\n",
      "180/180 [==============================] - 188s - loss: 0.0293 - acc: 0.9881 - val_loss: 0.0475 - val_acc: 0.9845\n",
      "Epoch 5/8\n",
      "180/180 [==============================] - 188s - loss: 0.0209 - acc: 0.9923 - val_loss: 0.0500 - val_acc: 0.9840\n",
      "Epoch 6/8\n",
      "180/180 [==============================] - 188s - loss: 0.0166 - acc: 0.9941 - val_loss: 0.0509 - val_acc: 0.9850\n",
      "Epoch 7/8\n",
      "180/180 [==============================] - 188s - loss: 0.0147 - acc: 0.9949 - val_loss: 0.0517 - val_acc: 0.9835\n",
      "Epoch 8/8\n",
      "180/180 [==============================] - 188s - loss: 0.0152 - acc: 0.9943 - val_loss: 0.0535 - val_acc: 0.9840\n",
      "Epoch 1/10\n",
      "180/180 [==============================] - 189s - loss: 0.0130 - acc: 0.9953 - val_loss: 0.0537 - val_acc: 0.9835\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 188s - loss: 0.0141 - acc: 0.9949 - val_loss: 0.0550 - val_acc: 0.9840\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 188s - loss: 0.0111 - acc: 0.9961 - val_loss: 0.0555 - val_acc: 0.9855\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 188s - loss: 0.0109 - acc: 0.9958 - val_loss: 0.0581 - val_acc: 0.9840\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 188s - loss: 0.0080 - acc: 0.9968 - val_loss: 0.0632 - val_acc: 0.9845\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 189s - loss: 0.0088 - acc: 0.9964 - val_loss: 0.0583 - val_acc: 0.9850\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 189s - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0610 - val_acc: 0.9840\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 189s - loss: 0.0077 - acc: 0.9977 - val_loss: 0.0571 - val_acc: 0.9855\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 188s - loss: 0.0057 - acc: 0.9978 - val_loss: 0.0604 - val_acc: 0.9845\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 189s - loss: 0.0058 - acc: 0.9977 - val_loss: 0.0643 - val_acc: 0.9855\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.8303 - acc: 0.6877 - val_loss: 2.2061 - val_acc: 0.3115\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6640 - acc: 0.7842 - val_loss: 2.4278 - val_acc: 0.3025\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6406 - acc: 0.7919 - val_loss: 2.4346 - val_acc: 0.2975\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6319 - acc: 0.7983 - val_loss: 2.4229 - val_acc: 0.2950\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6217 - acc: 0.8004 - val_loss: 2.3945 - val_acc: 0.2945\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6077 - acc: 0.8027 - val_loss: 2.3483 - val_acc: 0.2935\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6033 - acc: 0.8017 - val_loss: 2.3561 - val_acc: 0.2930\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5905 - acc: 0.8072 - val_loss: 2.3143 - val_acc: 0.2910\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5934 - acc: 0.8039 - val_loss: 2.2917 - val_acc: 0.2880\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5747 - acc: 0.8090 - val_loss: 2.3198 - val_acc: 0.2890\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5741 - acc: 0.8102 - val_loss: 2.2705 - val_acc: 0.2875\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5653 - acc: 0.8133 - val_loss: 2.2241 - val_acc: 0.2875\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 1s - loss: 0.5566 - acc: 0.8143 - val_loss: 2.2264 - val_acc: 0.2890\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0887 - acc: 0.9655 - val_loss: 0.0397 - val_acc: 0.9845\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0392 - acc: 0.9859 - val_loss: 0.0395 - val_acc: 0.9875\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 195s - loss: 0.0522 - acc: 0.9817 - val_loss: 0.0386 - val_acc: 0.9875\n",
      "Epoch 1/8\n",
      "180/180 [==============================] - 192s - loss: 0.0397 - acc: 0.9866 - val_loss: 0.0397 - val_acc: 0.9875\n",
      "Epoch 2/8\n",
      "180/180 [==============================] - 190s - loss: 0.0338 - acc: 0.9882 - val_loss: 0.0425 - val_acc: 0.9865\n",
      "Epoch 3/8\n",
      "180/180 [==============================] - 190s - loss: 0.0278 - acc: 0.9893 - val_loss: 0.0424 - val_acc: 0.9855\n",
      "Epoch 4/8\n",
      "180/180 [==============================] - 190s - loss: 0.0252 - acc: 0.9912 - val_loss: 0.0437 - val_acc: 0.9860\n",
      "Epoch 5/8\n",
      "180/180 [==============================] - 190s - loss: 0.0223 - acc: 0.9926 - val_loss: 0.0418 - val_acc: 0.9845\n",
      "Epoch 6/8\n",
      "180/180 [==============================] - 189s - loss: 0.0176 - acc: 0.9936 - val_loss: 0.0448 - val_acc: 0.9845\n",
      "Epoch 7/8\n",
      "180/180 [==============================] - 190s - loss: 0.0164 - acc: 0.9941 - val_loss: 0.0456 - val_acc: 0.9860\n",
      "Epoch 8/8\n",
      "180/180 [==============================] - 190s - loss: 0.0140 - acc: 0.9950 - val_loss: 0.0479 - val_acc: 0.9855\n",
      "Epoch 1/10\n",
      "180/180 [==============================] - 190s - loss: 0.0130 - acc: 0.9957 - val_loss: 0.0510 - val_acc: 0.9835\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 190s - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0510 - val_acc: 0.9850\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 190s - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0485 - val_acc: 0.9870\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 190s - loss: 0.0120 - acc: 0.9963 - val_loss: 0.0450 - val_acc: 0.9890\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 190s - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0459 - val_acc: 0.9865\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 190s - loss: 0.0084 - acc: 0.9971 - val_loss: 0.0535 - val_acc: 0.9855\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 190s - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0523 - val_acc: 0.9860\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 190s - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0535 - val_acc: 0.9855\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 190s - loss: 0.0077 - acc: 0.9971 - val_loss: 0.0560 - val_acc: 0.9855\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 190s - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0556 - val_acc: 0.9855\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.8412 - acc: 0.6823 - val_loss: 2.1991 - val_acc: 0.3060\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6874 - acc: 0.7768 - val_loss: 2.4586 - val_acc: 0.3010\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6574 - acc: 0.7957 - val_loss: 2.4415 - val_acc: 0.2980\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6379 - acc: 0.7957 - val_loss: 2.4440 - val_acc: 0.2935\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6262 - acc: 0.7996 - val_loss: 2.4113 - val_acc: 0.2910\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6145 - acc: 0.8032 - val_loss: 2.3883 - val_acc: 0.2895\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6072 - acc: 0.8072 - val_loss: 2.3196 - val_acc: 0.2895\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6018 - acc: 0.8054 - val_loss: 2.3292 - val_acc: 0.2905\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5941 - acc: 0.8071 - val_loss: 2.2962 - val_acc: 0.2895\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5850 - acc: 0.8074 - val_loss: 2.2776 - val_acc: 0.2890\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5698 - acc: 0.8134 - val_loss: 2.2468 - val_acc: 0.2885\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5631 - acc: 0.8119 - val_loss: 2.2374 - val_acc: 0.2890\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 1s - loss: 0.5538 - acc: 0.8135 - val_loss: 2.2233 - val_acc: 0.2895\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0885 - acc: 0.9681 - val_loss: 0.0356 - val_acc: 0.9875\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0411 - acc: 0.9853 - val_loss: 0.0365 - val_acc: 0.9870\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 195s - loss: 0.0515 - acc: 0.9824 - val_loss: 0.0356 - val_acc: 0.9865\n",
      "Epoch 1/8\n",
      "180/180 [==============================] - 192s - loss: 0.0361 - acc: 0.9864 - val_loss: 0.0382 - val_acc: 0.9855\n",
      "Epoch 2/8\n",
      "180/180 [==============================] - 190s - loss: 0.0331 - acc: 0.9879 - val_loss: 0.0393 - val_acc: 0.9865\n",
      "Epoch 3/8\n",
      "180/180 [==============================] - 190s - loss: 0.0288 - acc: 0.9895 - val_loss: 0.0387 - val_acc: 0.9865\n",
      "Epoch 4/8\n",
      "180/180 [==============================] - 190s - loss: 0.0266 - acc: 0.9904 - val_loss: 0.0412 - val_acc: 0.9875\n",
      "Epoch 5/8\n",
      "180/180 [==============================] - 190s - loss: 0.0198 - acc: 0.9929 - val_loss: 0.0419 - val_acc: 0.9870\n",
      "Epoch 6/8\n",
      "180/180 [==============================] - 190s - loss: 0.0166 - acc: 0.9936 - val_loss: 0.0421 - val_acc: 0.9865\n",
      "Epoch 7/8\n",
      "180/180 [==============================] - 190s - loss: 0.0139 - acc: 0.9947 - val_loss: 0.0426 - val_acc: 0.9880\n",
      "Epoch 8/8\n",
      "180/180 [==============================] - 190s - loss: 0.0125 - acc: 0.9955 - val_loss: 0.0447 - val_acc: 0.9890\n",
      "Epoch 1/10\n",
      "180/180 [==============================] - 190s - loss: 0.0147 - acc: 0.9948 - val_loss: 0.0465 - val_acc: 0.9880\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 190s - loss: 0.0120 - acc: 0.9956 - val_loss: 0.0505 - val_acc: 0.9870\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 190s - loss: 0.0103 - acc: 0.9962 - val_loss: 0.0509 - val_acc: 0.9875\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 189s - loss: 0.0106 - acc: 0.9962 - val_loss: 0.0502 - val_acc: 0.9875\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 190s - loss: 0.0079 - acc: 0.9970 - val_loss: 0.0515 - val_acc: 0.9870\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 189s - loss: 0.0073 - acc: 0.9977 - val_loss: 0.0518 - val_acc: 0.9880\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 189s - loss: 0.0070 - acc: 0.9972 - val_loss: 0.0485 - val_acc: 0.9865\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 189s - loss: 0.0065 - acc: 0.9975 - val_loss: 0.0546 - val_acc: 0.9860\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 189s - loss: 0.0062 - acc: 0.9978 - val_loss: 0.0551 - val_acc: 0.9855\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 189s - loss: 0.0067 - acc: 0.9975 - val_loss: 0.0572 - val_acc: 0.9875\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.8589 - acc: 0.6734 - val_loss: 2.1607 - val_acc: 0.3215\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6750 - acc: 0.7772 - val_loss: 2.4255 - val_acc: 0.3075\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6531 - acc: 0.7947 - val_loss: 2.4741 - val_acc: 0.3050\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6389 - acc: 0.7968 - val_loss: 2.4259 - val_acc: 0.3015\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6368 - acc: 0.7971 - val_loss: 2.4019 - val_acc: 0.3000\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6133 - acc: 0.8034 - val_loss: 2.4132 - val_acc: 0.2975\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6134 - acc: 0.8041 - val_loss: 2.3946 - val_acc: 0.2965\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5964 - acc: 0.8073 - val_loss: 2.3359 - val_acc: 0.2970\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5894 - acc: 0.8072 - val_loss: 2.2916 - val_acc: 0.2965\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5713 - acc: 0.8126 - val_loss: 2.3110 - val_acc: 0.2960\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5836 - acc: 0.8102 - val_loss: 2.2848 - val_acc: 0.2935\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5640 - acc: 0.8156 - val_loss: 2.2374 - val_acc: 0.2940\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 1s - loss: 0.5658 - acc: 0.8117 - val_loss: 2.2397 - val_acc: 0.2935\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0854 - acc: 0.9671 - val_loss: 0.0485 - val_acc: 0.9820\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0373 - acc: 0.9875 - val_loss: 0.0438 - val_acc: 0.9845\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 195s - loss: 0.0497 - acc: 0.9823 - val_loss: 0.0421 - val_acc: 0.9855\n",
      "Epoch 1/8\n",
      "180/180 [==============================] - 192s - loss: 0.0429 - acc: 0.9853 - val_loss: 0.0445 - val_acc: 0.9870\n",
      "Epoch 2/8\n",
      "180/180 [==============================] - 191s - loss: 0.0324 - acc: 0.9883 - val_loss: 0.0470 - val_acc: 0.9855\n",
      "Epoch 3/8\n",
      "180/180 [==============================] - 190s - loss: 0.0309 - acc: 0.9896 - val_loss: 0.0489 - val_acc: 0.9845\n",
      "Epoch 4/8\n",
      "180/180 [==============================] - 190s - loss: 0.0222 - acc: 0.9925 - val_loss: 0.0469 - val_acc: 0.9855\n",
      "Epoch 5/8\n",
      "180/180 [==============================] - 190s - loss: 0.0210 - acc: 0.9929 - val_loss: 0.0482 - val_acc: 0.9850\n",
      "Epoch 6/8\n",
      "180/180 [==============================] - 190s - loss: 0.0189 - acc: 0.9932 - val_loss: 0.0478 - val_acc: 0.9850\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 190s - loss: 0.0162 - acc: 0.9941 - val_loss: 0.0537 - val_acc: 0.9850\n",
      "Epoch 8/8\n",
      "180/180 [==============================] - 190s - loss: 0.0140 - acc: 0.9952 - val_loss: 0.0532 - val_acc: 0.9855\n",
      "Epoch 1/10\n",
      "180/180 [==============================] - 191s - loss: 0.0142 - acc: 0.9954 - val_loss: 0.0530 - val_acc: 0.9870\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 190s - loss: 0.0131 - acc: 0.9957 - val_loss: 0.0544 - val_acc: 0.9865\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 190s - loss: 0.0125 - acc: 0.9956 - val_loss: 0.0573 - val_acc: 0.9870\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 190s - loss: 0.0102 - acc: 0.9962 - val_loss: 0.0582 - val_acc: 0.9855\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 190s - loss: 0.0097 - acc: 0.9967 - val_loss: 0.0603 - val_acc: 0.9870\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 190s - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0587 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 190s - loss: 0.0084 - acc: 0.9968 - val_loss: 0.0574 - val_acc: 0.9865\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 190s - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0590 - val_acc: 0.9865\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 190s - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0582 - val_acc: 0.9870\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 190s - loss: 0.0060 - acc: 0.9977 - val_loss: 0.0574 - val_acc: 0.9865\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.9131 - acc: 0.6537 - val_loss: 2.0994 - val_acc: 0.3305\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6903 - acc: 0.7715 - val_loss: 2.3648 - val_acc: 0.3140\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6595 - acc: 0.7897 - val_loss: 2.4040 - val_acc: 0.3130\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6486 - acc: 0.7928 - val_loss: 2.4055 - val_acc: 0.3090\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6353 - acc: 0.7966 - val_loss: 2.3692 - val_acc: 0.3075\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6194 - acc: 0.8024 - val_loss: 2.3367 - val_acc: 0.3060\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.6058 - acc: 0.8050 - val_loss: 2.3045 - val_acc: 0.3030\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5981 - acc: 0.8070 - val_loss: 2.2835 - val_acc: 0.3000\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5824 - acc: 0.8097 - val_loss: 2.2697 - val_acc: 0.2990\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5752 - acc: 0.8097 - val_loss: 2.2390 - val_acc: 0.2980\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5702 - acc: 0.8128 - val_loss: 2.1972 - val_acc: 0.2995\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 1s - loss: 0.5630 - acc: 0.8120 - val_loss: 2.1957 - val_acc: 0.2960\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 1s - loss: 0.5530 - acc: 0.8130 - val_loss: 2.1798 - val_acc: 0.2960\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0872 - acc: 0.9678 - val_loss: 0.0453 - val_acc: 0.9835\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 9s - loss: 0.0370 - acc: 0.9866 - val_loss: 0.0459 - val_acc: 0.9825\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 194s - loss: 0.0490 - acc: 0.9816 - val_loss: 0.0495 - val_acc: 0.9835\n",
      "Epoch 1/8\n",
      "180/180 [==============================] - 193s - loss: 0.0357 - acc: 0.9870 - val_loss: 0.0466 - val_acc: 0.9840\n",
      "Epoch 2/8\n",
      "180/180 [==============================] - 191s - loss: 0.0312 - acc: 0.9893 - val_loss: 0.0475 - val_acc: 0.9850\n",
      "Epoch 3/8\n",
      "180/180 [==============================] - 190s - loss: 0.0271 - acc: 0.9901 - val_loss: 0.0489 - val_acc: 0.9855\n",
      "Epoch 4/8\n",
      "180/180 [==============================] - 190s - loss: 0.0236 - acc: 0.9910 - val_loss: 0.0478 - val_acc: 0.9855\n",
      "Epoch 5/8\n",
      "180/180 [==============================] - 191s - loss: 0.0194 - acc: 0.9926 - val_loss: 0.0459 - val_acc: 0.9840\n",
      "Epoch 6/8\n",
      "180/180 [==============================] - 190s - loss: 0.0147 - acc: 0.9949 - val_loss: 0.0475 - val_acc: 0.9835\n",
      "Epoch 7/8\n",
      "180/180 [==============================] - 190s - loss: 0.0160 - acc: 0.9951 - val_loss: 0.0507 - val_acc: 0.9850\n",
      "Epoch 8/8\n",
      "180/180 [==============================] - 190s - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0519 - val_acc: 0.9830\n",
      "Epoch 1/10\n",
      "180/180 [==============================] - 191s - loss: 0.0137 - acc: 0.9949 - val_loss: 0.0516 - val_acc: 0.9860\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 191s - loss: 0.0117 - acc: 0.9957 - val_loss: 0.0511 - val_acc: 0.9860\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 191s - loss: 0.0094 - acc: 0.9969 - val_loss: 0.0530 - val_acc: 0.9845\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 191s - loss: 0.0100 - acc: 0.9966 - val_loss: 0.0565 - val_acc: 0.9855\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 191s - loss: 0.0106 - acc: 0.9963 - val_loss: 0.0550 - val_acc: 0.9865\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 191s - loss: 0.0078 - acc: 0.9972 - val_loss: 0.0574 - val_acc: 0.9875\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 190s - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0595 - val_acc: 0.9855\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 191s - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0584 - val_acc: 0.9845\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 191s - loss: 0.0062 - acc: 0.9976 - val_loss: 0.0630 - val_acc: 0.9850\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 191s - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0592 - val_acc: 0.9850\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    i = str(i)\n",
    "    model = train_last_layer(i)\n",
    "    train_dense_layers(i, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ensemble and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ens_model = vgg_ft_bn(2)\n",
    "for layer in ens_model.layers: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ens_pred(arr, fname):\n",
    "    ens_pred = []\n",
    "    for i in range(5):\n",
    "        i = str(i)\n",
    "        ens_model.load_weights('{}{}{}.h5'.format(model_path, fname, i))\n",
    "        preds = ens_model.predict(arr, batch_size=batch_size)\n",
    "        ens_pred.append(preds)\n",
    "    return ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred2 = get_ens_pred(val, 'aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_avg_preds2 = np.stack(val_pred2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98650002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_accuracy(val_labels, val_avg_preds2).eval().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "p3",
   "language": "python",
   "name": "p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
